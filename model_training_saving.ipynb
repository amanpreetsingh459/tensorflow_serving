{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed7ce53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2.5.0\n",
    "#!pip install tensorflow_datasets\n",
    "#!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a2e454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 18:13:02.023083: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-12 18:13:02.023134: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be6c9b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13c4c7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n",
      "2021-12-12 18:13:32.224578: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-12 18:13:32.224619: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-12 18:13:32.224648: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (amanpreet-singh): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "imdb, info = tfds.load(\"imdb_reviews/subwords8k\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e456bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = imdb['train']\n",
    "test_data = imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "573498fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd2d2a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['train', 'test', 'unsupervised'])\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n",
      "<class 'tensorflow_datasets.core.dataset_info.DatasetInfo'>\n",
      "<class 'tensorflow_datasets.core.features.features_dict.FeaturesDict'>\n",
      "dict_keys(['text', 'label'])\n",
      "Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>)\n",
      "<SubwordTextEncoder vocab_size=8185>\n",
      "8185\n"
     ]
    }
   ],
   "source": [
    "print(type(imdb))\n",
    "print(imdb.keys())\n",
    "print(type(imdb['train']))\n",
    "print(type(info))\n",
    "print(type(info.features))\n",
    "print(info.features.keys())\n",
    "print(info.features['text'])\n",
    "print(info.features['text'].encoder)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c73d820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test string:  That movie i saw the other day was fantabulous\n",
      "tokenized string:  [444, 27, 131, 284, 1, 108, 414, 18, 1042, 4779, 5872]\n",
      "decoded string:  That movie i saw the other day was fantabulous\n"
     ]
    }
   ],
   "source": [
    "test_string = \"That movie i saw the other day was fantabulous\"\n",
    "print(\"test string: \", test_string)\n",
    "tokenized_string = tokenizer.encode(test_string)\n",
    "print(\"tokenized string: \", tokenized_string)\n",
    "decoded_string = tokenizer.decode(tokenized_string)\n",
    "print(\"decoded string: \", decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "154e0b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 ----> That \n",
      "27 ----> movie \n",
      "131 ----> i \n",
      "284 ----> saw \n",
      "1 ----> the \n",
      "108 ----> other \n",
      "414 ----> day \n",
      "18 ----> was \n",
      "1042 ----> fan\n",
      "4779 ----> tab\n",
      "5872 ----> ulous\n"
     ]
    }
   ],
   "source": [
    "for token in tokenized_string:\n",
    "  print('{} ----> {}'.format(token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eb89ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the tokenizer\n",
    "import pickle\n",
    "with open('tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f87eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_data.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_dataset))\n",
    "test_dataset = test_data.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9209c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b7d670f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          523840    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 524,237\n",
      "Trainable params: 524,237\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19ac8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13495fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0877f71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 13:52:48.697181: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-12-12 13:52:48.733025: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2394800000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 25s 56ms/step - loss: 0.6635 - accuracy: 0.6318 - val_loss: 0.5981 - val_accuracy: 0.7801\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4852 - accuracy: 0.8232 - val_loss: 0.4168 - val_accuracy: 0.8527\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3450 - accuracy: 0.8769 - val_loss: 0.3453 - val_accuracy: 0.8708\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2818 - accuracy: 0.8999 - val_loss: 0.3188 - val_accuracy: 0.8768\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.2478 - accuracy: 0.9119 - val_loss: 0.3102 - val_accuracy: 0.8778\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2234 - accuracy: 0.9198 - val_loss: 0.3110 - val_accuracy: 0.8770\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 0.2046 - accuracy: 0.9268 - val_loss: 0.3208 - val_accuracy: 0.8706\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 0.1870 - accuracy: 0.9350 - val_loss: 0.3269 - val_accuracy: 0.8711\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 0.1775 - accuracy: 0.9379 - val_loss: 0.3304 - val_accuracy: 0.8725\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1631 - accuracy: 0.9433 - val_loss: 0.3210 - val_accuracy: 0.8790\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03e0c8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test an examples on the trained model\n",
    "test_review = \"That movie i saw the other day was fantastic\"\n",
    "tokenized_string = tokenizer.encode(test_review)\n",
    "import numpy as np\n",
    "prediction = model.predict(tokenized_string)\n",
    "prediction[np.argmax(prediction)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa38fa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 14:07:43.563816: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentiments_classifier/1639298263/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentiments_classifier/1639298263/assets\n"
     ]
    }
   ],
   "source": [
    "## save the model into well defined directory\n",
    "import time\n",
    "ts = int(time.time())\n",
    "file_path = f\"sentiment_classifier/{ts}/\"\n",
    "model.save(filepath=file_path, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a56729c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
